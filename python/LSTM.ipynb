{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Home/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/Home/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from sql_to_df import retrieve_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = retrieve_data(\"AAPL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            adj_open_price  adj_close_price  adj_high_price  adj_low_price\n",
      "price_date                                                                \n",
      "2000-01-03          3.3693           3.5965          3.6145         3.2671\n",
      "2000-01-04          3.4779           3.2932          3.5541         3.2511\n",
      "2000-01-05          3.3333           3.3414          3.5521         3.3092\n",
      "2000-01-06          3.4095           3.0522          3.4377         3.0522\n",
      "2000-01-07          3.1004           3.1968          3.2450         3.0683\n",
      "2000-01-10          3.2771           3.1406          3.2851         3.0442\n",
      "2000-01-11          3.0824           2.9799          3.1926         2.9076\n",
      "2000-01-12          3.0522           2.8013          3.0683         2.7791\n",
      "2000-01-13          3.0355           3.1084          3.1727         2.9719\n",
      "2000-01-14          3.2128           3.2270          3.2851         3.1926\n",
      "2000-01-18          3.2450           3.3394          3.4056         3.2270\n",
      "2000-01-19          3.3934           3.4236          3.4940         3.3211\n",
      "2000-01-20          3.7108           3.6466          3.9036         3.6466\n",
      "2000-01-21          3.6707           3.5762          3.6707         3.5402\n",
      "2000-01-24          3.4840           3.4137          3.6225         3.3773\n",
      "2000-01-25          3.3735           3.6064          3.6344         3.2890\n",
      "2000-01-26          3.5341           3.5402          3.6688         3.5261\n",
      "2000-01-27          3.4959           3.5341          3.6305         3.4377\n",
      "2000-01-28          3.4760           3.2649          3.5621         3.2328\n",
      "2000-01-31          3.2450           3.3333          3.3372         3.0361\n",
      "2000-02-01          3.3414           3.2209          3.3735         3.2128\n",
      "2000-02-02          3.2369           3.1746          3.2810         3.1165\n",
      "2000-02-03          3.2228           3.3192          3.3494         3.2209\n",
      "2000-02-04          3.3394           3.4699          3.5341         3.3292\n",
      "2000-02-07          3.4699           3.6646          3.6707         3.4037\n",
      "2000-02-08          3.6626           3.6906          3.7308         3.5743\n",
      "2000-02-09          3.6665           3.6183          3.7629         3.6125\n",
      "2000-02-10          3.6263           3.6466          3.6585         3.5341\n",
      "2000-02-11          3.6504           3.4940          3.6665         3.4779\n",
      "2000-02-14          3.5120           3.7208          3.7227         3.4898\n",
      "...                    ...              ...             ...            ...\n",
      "2018-02-13        161.9500         164.3400        164.7500       161.6500\n",
      "2018-02-14        163.0450         167.3700        167.5400       162.8800\n",
      "2018-02-15        169.7900         172.9900        173.0900       169.0000\n",
      "2018-02-16        172.3600         172.4300        174.8200       171.7700\n",
      "2018-02-20        172.0500         171.8500        174.2600       171.4200\n",
      "2018-02-21        172.8300         171.0700        174.1200       171.0100\n",
      "2018-02-22        171.8000         172.6000        173.9500       171.7100\n",
      "2018-02-23        173.6700         175.5550        175.6500       173.5400\n",
      "2018-02-26        176.3500         178.9700        179.3900       176.2100\n",
      "2018-02-27        179.1000         178.3900        180.4800       178.1600\n",
      "2018-02-28        179.2600         178.1200        180.6150       178.0500\n",
      "2018-03-01        178.5400         175.0000        179.7750       172.6600\n",
      "2018-03-02        172.8000         176.2100        176.3000       172.4500\n",
      "2018-03-05        175.2100         176.8200        177.7400       174.5200\n",
      "2018-03-06        177.9100         176.6700        178.2500       176.1300\n",
      "2018-03-07        174.9400         175.0300        175.8500       174.2700\n",
      "2018-03-08        175.4800         176.9400        177.1200       175.0700\n",
      "2018-03-09        177.9600         179.9800        180.0000       177.3900\n",
      "2018-03-12        180.2900         181.7200        182.3900       180.2100\n",
      "2018-03-13        182.5900         179.9700        183.5000       179.2400\n",
      "2018-03-14        180.3200         178.4400        180.5200       177.8100\n",
      "2018-03-15        178.5000         178.6500        180.2400       178.0701\n",
      "2018-03-16        178.6500         178.0200        179.1200       177.6200\n",
      "2018-03-19        177.3200         175.3000        177.4700       173.6600\n",
      "2018-03-20        175.2400         175.2400        176.8000       174.9400\n",
      "2018-03-21        175.0400         171.2700        175.0900       171.2600\n",
      "2018-03-22        170.0000         168.8450        172.6800       168.6000\n",
      "2018-03-23        168.3900         164.9400        169.9200       164.9400\n",
      "2018-03-26        168.0700         172.7700        173.1000       166.4400\n",
      "2018-03-27        173.6800         168.3400        175.1500       166.9200\n",
      "\n",
      "[4585 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(stock, seq_len):\n",
    "    amount_of_features = len(stock.columns)\n",
    "    data = stock.as_matrix() #pd.DataFrame(stock)\n",
    "    sequence_length = seq_len + 1\n",
    "    result = []\n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result.append(data[index: index + sequence_length])\n",
    "        \n",
    "    result = np.array(result)\n",
    "    train_rows = round(0.9 * result.shape[0])\n",
    "    train = result[:int(train_rows), :]\n",
    "    x_train = train[:, :-1]\n",
    "    y_train = train[:, -1][:,-1]\n",
    "    x_test = result[int(train_rows):, :-1]\n",
    "    y_test = result[int(train_rows):, -1][:,-1]\n",
    "\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], amount_of_features))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], amount_of_features))  \n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        input_dim=layers[0],\n",
    "        output_dim=layers[1],\n",
    "        return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "        layers[2],\n",
    "        return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(\n",
    "        output_dim=layers[2]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    start = time.time()\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\",metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model\n",
    "\n",
    "def build_model2(layers):\n",
    "        d = 0.2\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "        model.add(Dropout(d))\n",
    "        model.add(LSTM(64, input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "        model.add(Dropout(d))\n",
    "        model.add(Dense(16,init='uniform',activation='relu'))        \n",
    "        model.add(Dense(1,init='uniform',activation='linear'))\n",
    "        model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (4117, 10, 4)\n",
      "y_train (4117,)\n",
      "X_test (457, 10, 4)\n",
      "y_test (457,)\n"
     ]
    }
   ],
   "source": [
    "window = 10\n",
    "X_train, y_train, X_test, y_test = load_data(df, window)\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model([3,lag,1])\n",
    "model = build_model2([4,window,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Home/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(16, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "/Users/Home/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"linear\", kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "layers = [4,window,1]\n",
    "\n",
    "d = 0.2\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "model.add(Dropout(d))\n",
    "model.add(LSTM(64, input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "model.add(Dropout(d))\n",
    "model.add(Dense(16,init='uniform',activation='relu'))        \n",
    "model.add(Dense(1,init='uniform',activation='linear'))\n",
    "model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=512,\n",
    "    nb_epoch=10,\n",
    "    validation_split=0.1,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(X_test[-1])\n",
    "diff=[]\n",
    "ratio=[]\n",
    "p = model.predict(X_test)\n",
    "for u in range(len(y_test)):\n",
    "    pr = p[u][0]\n",
    "    ratio.append((y_test[u]/pr)-1)\n",
    "    diff.append(abs(y_test[u]- pr))\n",
    "    #print(u, y_test[u], pr, (y_test[u]/pr)-1, abs(y_test[u]- pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt2\n",
    "\n",
    "plt2.plot(p,color='red', label='prediction')\n",
    "plt2.plot(y_test,color='blue', label='y_test')\n",
    "plt2.legend(loc='upper left')\n",
    "plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
