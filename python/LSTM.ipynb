{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note:\n",
    "I have some major hestitations about using too much machine learning for\n",
    "financial data. I think there is a sort of blindness that happens when ml is\n",
    "relied upon too heavily, and I think that rnn's are bound to overfit the past\n",
    "and are extremely succeptible to black swan events. I would be very cautious\n",
    "when using LSTM's for trading.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Home/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/Home/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from sql_to_df import retrieve_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = retrieve_data(\"AAPL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            adj_open_price  adj_close_price  adj_high_price  adj_low_price\n",
      "price_date                                                                \n",
      "2000-01-03       4783900.0           3.2671          3.3693         3.6145\n",
      "2000-01-04       4574800.0           3.2511          3.4779         3.5541\n",
      "2000-01-05       6949300.0           3.3092          3.3333         3.5521\n",
      "2000-01-06       6856900.0           3.0522          3.4095         3.4377\n",
      "2000-01-07       4113700.0           3.0683          3.1004         3.2450\n",
      "2000-01-10       4509500.0           3.0442          3.2771         3.2851\n",
      "2000-01-11       3942400.0           2.9076          3.0824         3.1926\n",
      "2000-01-12       8714900.0           2.7791          3.0522         3.0683\n",
      "2000-01-13       9220400.0           2.9719          3.0355         3.1727\n",
      "2000-01-14       3485500.0           3.1926          3.2128         3.2851\n",
      "2000-01-18       4099800.0           3.2270          3.2450         3.4056\n",
      "2000-01-19       5336100.0           3.3211          3.3934         3.4940\n",
      "2000-01-20      16349400.0           3.6466          3.7108         3.9036\n",
      "2000-01-21       4427900.0           3.5402          3.6707         3.6707\n",
      "2000-01-24       3936400.0           3.3773          3.4840         3.6225\n",
      "2000-01-25       4438800.0           3.2890          3.3735         3.6344\n",
      "2000-01-26       3278200.0           3.5261          3.5341         3.6688\n",
      "2000-01-27       3037000.0           3.4377          3.4959         3.6305\n",
      "2000-01-28       3779900.0           3.2328          3.4760         3.5621\n",
      "2000-01-31       6265000.0           3.0361          3.2450         3.3372\n",
      "2000-02-01       2839600.0           3.2128          3.3414         3.3735\n",
      "2000-02-02       4144600.0           3.1165          3.2369         3.2810\n",
      "2000-02-03       4242800.0           3.2209          3.2228         3.3494\n",
      "2000-02-04       3797500.0           3.3292          3.3394         3.5341\n",
      "2000-02-07       3938100.0           3.4037          3.4699         3.6707\n",
      "2000-02-08       3648600.0           3.5743          3.6626         3.7308\n",
      "2000-02-09       2672900.0           3.6125          3.6665         3.7629\n",
      "2000-02-10       2705200.0           3.5341          3.6263         3.6585\n",
      "2000-02-11       1895100.0           3.4779          3.6504         3.6665\n",
      "2000-02-14       3281600.0           3.4898          3.5120         3.7227\n",
      "...                    ...              ...             ...            ...\n",
      "2018-02-13      32104756.0         161.6500        161.9500       164.7500\n",
      "2018-02-14      39669178.0         162.8800        163.0450       167.5400\n",
      "2018-02-15      50609595.0         169.0000        169.7900       173.0900\n",
      "2018-02-16      39638793.0         171.7700        172.3600       174.8200\n",
      "2018-02-20      33531012.0         171.4200        172.0500       174.2600\n",
      "2018-02-21      35833514.0         171.0100        172.8300       174.1200\n",
      "2018-02-22      30504116.0         171.7100        171.8000       173.9500\n",
      "2018-02-23      33329232.0         173.5400        173.6700       175.6500\n",
      "2018-02-26      36886432.0         176.2100        176.3500       179.3900\n",
      "2018-02-27      38685165.0         178.1600        179.1000       180.4800\n",
      "2018-02-28      33604574.0         178.0500        179.2600       180.6150\n",
      "2018-03-01      48801970.0         172.6600        178.5400       179.7750\n",
      "2018-03-02      38453950.0         172.4500        172.8000       176.3000\n",
      "2018-03-05      28401366.0         174.5200        175.2100       177.7400\n",
      "2018-03-06      23788506.0         176.1300        177.9100       178.2500\n",
      "2018-03-07      31703462.0         174.2700        174.9400       175.8500\n",
      "2018-03-08      23163767.0         175.0700        175.4800       177.1200\n",
      "2018-03-09      31385134.0         177.3900        177.9600       180.0000\n",
      "2018-03-12      32055405.0         180.2100        180.2900       182.3900\n",
      "2018-03-13      31168404.0         179.2400        182.5900       183.5000\n",
      "2018-03-14      29075469.0         177.8100        180.3200       180.5200\n",
      "2018-03-15      22584565.0         178.0701        178.5000       180.2400\n",
      "2018-03-16      36836456.0         177.6200        178.6500       179.1200\n",
      "2018-03-19      32804695.0         173.6600        177.3200       177.4700\n",
      "2018-03-20      19314039.0         174.9400        175.2400       176.8000\n",
      "2018-03-21      35247358.0         171.2600        175.0400       175.0900\n",
      "2018-03-22      41051076.0         168.6000        170.0000       172.6800\n",
      "2018-03-23      40248954.0         164.9400        168.3900       169.9200\n",
      "2018-03-26      36272617.0         166.4400        168.0700       173.1000\n",
      "2018-03-27      38962839.0         166.9200        173.6800       175.1500\n",
      "\n",
      "[4585 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>adj_open_price</th>\n",
       "      <th>adj_close_price</th>\n",
       "      <th>adj_high_price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03 00:00:00</th>\n",
       "      <td>4783900.0</td>\n",
       "      <td>3.2671</td>\n",
       "      <td>3.3693</td>\n",
       "      <td>3.6145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04 00:00:00</th>\n",
       "      <td>4574800.0</td>\n",
       "      <td>3.2511</td>\n",
       "      <td>3.4779</td>\n",
       "      <td>3.5541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05 00:00:00</th>\n",
       "      <td>6949300.0</td>\n",
       "      <td>3.3092</td>\n",
       "      <td>3.3333</td>\n",
       "      <td>3.5521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06 00:00:00</th>\n",
       "      <td>6856900.0</td>\n",
       "      <td>3.0522</td>\n",
       "      <td>3.4095</td>\n",
       "      <td>3.4377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07 00:00:00</th>\n",
       "      <td>4113700.0</td>\n",
       "      <td>3.0683</td>\n",
       "      <td>3.1004</td>\n",
       "      <td>3.2450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-10 00:00:00</th>\n",
       "      <td>4509500.0</td>\n",
       "      <td>3.0442</td>\n",
       "      <td>3.2771</td>\n",
       "      <td>3.2851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-11 00:00:00</th>\n",
       "      <td>3942400.0</td>\n",
       "      <td>2.9076</td>\n",
       "      <td>3.0824</td>\n",
       "      <td>3.1926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-12 00:00:00</th>\n",
       "      <td>8714900.0</td>\n",
       "      <td>2.7791</td>\n",
       "      <td>3.0522</td>\n",
       "      <td>3.0683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-13 00:00:00</th>\n",
       "      <td>9220400.0</td>\n",
       "      <td>2.9719</td>\n",
       "      <td>3.0355</td>\n",
       "      <td>3.1727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-14 00:00:00</th>\n",
       "      <td>3485500.0</td>\n",
       "      <td>3.1926</td>\n",
       "      <td>3.2128</td>\n",
       "      <td>3.2851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-18 00:00:00</th>\n",
       "      <td>4099800.0</td>\n",
       "      <td>3.2270</td>\n",
       "      <td>3.2450</td>\n",
       "      <td>3.4056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-19 00:00:00</th>\n",
       "      <td>5336100.0</td>\n",
       "      <td>3.3211</td>\n",
       "      <td>3.3934</td>\n",
       "      <td>3.4940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-20 00:00:00</th>\n",
       "      <td>16349400.0</td>\n",
       "      <td>3.6466</td>\n",
       "      <td>3.7108</td>\n",
       "      <td>3.9036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-21 00:00:00</th>\n",
       "      <td>4427900.0</td>\n",
       "      <td>3.5402</td>\n",
       "      <td>3.6707</td>\n",
       "      <td>3.6707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-24 00:00:00</th>\n",
       "      <td>3936400.0</td>\n",
       "      <td>3.3773</td>\n",
       "      <td>3.4840</td>\n",
       "      <td>3.6225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-25 00:00:00</th>\n",
       "      <td>4438800.0</td>\n",
       "      <td>3.2890</td>\n",
       "      <td>3.3735</td>\n",
       "      <td>3.6344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-26 00:00:00</th>\n",
       "      <td>3278200.0</td>\n",
       "      <td>3.5261</td>\n",
       "      <td>3.5341</td>\n",
       "      <td>3.6688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-27 00:00:00</th>\n",
       "      <td>3037000.0</td>\n",
       "      <td>3.4377</td>\n",
       "      <td>3.4959</td>\n",
       "      <td>3.6305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-28 00:00:00</th>\n",
       "      <td>3779900.0</td>\n",
       "      <td>3.2328</td>\n",
       "      <td>3.4760</td>\n",
       "      <td>3.5621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-31 00:00:00</th>\n",
       "      <td>6265000.0</td>\n",
       "      <td>3.0361</td>\n",
       "      <td>3.2450</td>\n",
       "      <td>3.3372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-01 00:00:00</th>\n",
       "      <td>2839600.0</td>\n",
       "      <td>3.2128</td>\n",
       "      <td>3.3414</td>\n",
       "      <td>3.3735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-02 00:00:00</th>\n",
       "      <td>4144600.0</td>\n",
       "      <td>3.1165</td>\n",
       "      <td>3.2369</td>\n",
       "      <td>3.2810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-03 00:00:00</th>\n",
       "      <td>4242800.0</td>\n",
       "      <td>3.2209</td>\n",
       "      <td>3.2228</td>\n",
       "      <td>3.3494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-04 00:00:00</th>\n",
       "      <td>3797500.0</td>\n",
       "      <td>3.3292</td>\n",
       "      <td>3.3394</td>\n",
       "      <td>3.5341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-07 00:00:00</th>\n",
       "      <td>3938100.0</td>\n",
       "      <td>3.4037</td>\n",
       "      <td>3.4699</td>\n",
       "      <td>3.6707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-08 00:00:00</th>\n",
       "      <td>3648600.0</td>\n",
       "      <td>3.5743</td>\n",
       "      <td>3.6626</td>\n",
       "      <td>3.7308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-09 00:00:00</th>\n",
       "      <td>2672900.0</td>\n",
       "      <td>3.6125</td>\n",
       "      <td>3.6665</td>\n",
       "      <td>3.7629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-10 00:00:00</th>\n",
       "      <td>2705200.0</td>\n",
       "      <td>3.5341</td>\n",
       "      <td>3.6263</td>\n",
       "      <td>3.6585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-11 00:00:00</th>\n",
       "      <td>1895100.0</td>\n",
       "      <td>3.4779</td>\n",
       "      <td>3.6504</td>\n",
       "      <td>3.6665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-14 00:00:00</th>\n",
       "      <td>3281600.0</td>\n",
       "      <td>3.4898</td>\n",
       "      <td>3.5120</td>\n",
       "      <td>3.7227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-13 00:00:00</th>\n",
       "      <td>32104756.0</td>\n",
       "      <td>161.6500</td>\n",
       "      <td>161.9500</td>\n",
       "      <td>164.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-14 00:00:00</th>\n",
       "      <td>39669178.0</td>\n",
       "      <td>162.8800</td>\n",
       "      <td>163.0450</td>\n",
       "      <td>167.5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-15 00:00:00</th>\n",
       "      <td>50609595.0</td>\n",
       "      <td>169.0000</td>\n",
       "      <td>169.7900</td>\n",
       "      <td>173.0900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-16 00:00:00</th>\n",
       "      <td>39638793.0</td>\n",
       "      <td>171.7700</td>\n",
       "      <td>172.3600</td>\n",
       "      <td>174.8200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-20 00:00:00</th>\n",
       "      <td>33531012.0</td>\n",
       "      <td>171.4200</td>\n",
       "      <td>172.0500</td>\n",
       "      <td>174.2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-21 00:00:00</th>\n",
       "      <td>35833514.0</td>\n",
       "      <td>171.0100</td>\n",
       "      <td>172.8300</td>\n",
       "      <td>174.1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-22 00:00:00</th>\n",
       "      <td>30504116.0</td>\n",
       "      <td>171.7100</td>\n",
       "      <td>171.8000</td>\n",
       "      <td>173.9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-23 00:00:00</th>\n",
       "      <td>33329232.0</td>\n",
       "      <td>173.5400</td>\n",
       "      <td>173.6700</td>\n",
       "      <td>175.6500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-26 00:00:00</th>\n",
       "      <td>36886432.0</td>\n",
       "      <td>176.2100</td>\n",
       "      <td>176.3500</td>\n",
       "      <td>179.3900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-27 00:00:00</th>\n",
       "      <td>38685165.0</td>\n",
       "      <td>178.1600</td>\n",
       "      <td>179.1000</td>\n",
       "      <td>180.4800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28 00:00:00</th>\n",
       "      <td>33604574.0</td>\n",
       "      <td>178.0500</td>\n",
       "      <td>179.2600</td>\n",
       "      <td>180.6150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01 00:00:00</th>\n",
       "      <td>48801970.0</td>\n",
       "      <td>172.6600</td>\n",
       "      <td>178.5400</td>\n",
       "      <td>179.7750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-02 00:00:00</th>\n",
       "      <td>38453950.0</td>\n",
       "      <td>172.4500</td>\n",
       "      <td>172.8000</td>\n",
       "      <td>176.3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-05 00:00:00</th>\n",
       "      <td>28401366.0</td>\n",
       "      <td>174.5200</td>\n",
       "      <td>175.2100</td>\n",
       "      <td>177.7400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-06 00:00:00</th>\n",
       "      <td>23788506.0</td>\n",
       "      <td>176.1300</td>\n",
       "      <td>177.9100</td>\n",
       "      <td>178.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-07 00:00:00</th>\n",
       "      <td>31703462.0</td>\n",
       "      <td>174.2700</td>\n",
       "      <td>174.9400</td>\n",
       "      <td>175.8500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-08 00:00:00</th>\n",
       "      <td>23163767.0</td>\n",
       "      <td>175.0700</td>\n",
       "      <td>175.4800</td>\n",
       "      <td>177.1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-09 00:00:00</th>\n",
       "      <td>31385134.0</td>\n",
       "      <td>177.3900</td>\n",
       "      <td>177.9600</td>\n",
       "      <td>180.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-12 00:00:00</th>\n",
       "      <td>32055405.0</td>\n",
       "      <td>180.2100</td>\n",
       "      <td>180.2900</td>\n",
       "      <td>182.3900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-13 00:00:00</th>\n",
       "      <td>31168404.0</td>\n",
       "      <td>179.2400</td>\n",
       "      <td>182.5900</td>\n",
       "      <td>183.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-14 00:00:00</th>\n",
       "      <td>29075469.0</td>\n",
       "      <td>177.8100</td>\n",
       "      <td>180.3200</td>\n",
       "      <td>180.5200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-15 00:00:00</th>\n",
       "      <td>22584565.0</td>\n",
       "      <td>178.0701</td>\n",
       "      <td>178.5000</td>\n",
       "      <td>180.2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-16 00:00:00</th>\n",
       "      <td>36836456.0</td>\n",
       "      <td>177.6200</td>\n",
       "      <td>178.6500</td>\n",
       "      <td>179.1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-19 00:00:00</th>\n",
       "      <td>32804695.0</td>\n",
       "      <td>173.6600</td>\n",
       "      <td>177.3200</td>\n",
       "      <td>177.4700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-20 00:00:00</th>\n",
       "      <td>19314039.0</td>\n",
       "      <td>174.9400</td>\n",
       "      <td>175.2400</td>\n",
       "      <td>176.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-21 00:00:00</th>\n",
       "      <td>35247358.0</td>\n",
       "      <td>171.2600</td>\n",
       "      <td>175.0400</td>\n",
       "      <td>175.0900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-22 00:00:00</th>\n",
       "      <td>41051076.0</td>\n",
       "      <td>168.6000</td>\n",
       "      <td>170.0000</td>\n",
       "      <td>172.6800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-23 00:00:00</th>\n",
       "      <td>40248954.0</td>\n",
       "      <td>164.9400</td>\n",
       "      <td>168.3900</td>\n",
       "      <td>169.9200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-26 00:00:00</th>\n",
       "      <td>36272617.0</td>\n",
       "      <td>166.4400</td>\n",
       "      <td>168.0700</td>\n",
       "      <td>173.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-27 00:00:00</th>\n",
       "      <td>38962839.0</td>\n",
       "      <td>166.9200</td>\n",
       "      <td>173.6800</td>\n",
       "      <td>175.1500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4585 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         volume  adj_open_price  adj_close_price  \\\n",
       "price_date                                                         \n",
       "2000-01-03 00:00:00   4783900.0          3.2671           3.3693   \n",
       "2000-01-04 00:00:00   4574800.0          3.2511           3.4779   \n",
       "2000-01-05 00:00:00   6949300.0          3.3092           3.3333   \n",
       "2000-01-06 00:00:00   6856900.0          3.0522           3.4095   \n",
       "2000-01-07 00:00:00   4113700.0          3.0683           3.1004   \n",
       "2000-01-10 00:00:00   4509500.0          3.0442           3.2771   \n",
       "2000-01-11 00:00:00   3942400.0          2.9076           3.0824   \n",
       "2000-01-12 00:00:00   8714900.0          2.7791           3.0522   \n",
       "2000-01-13 00:00:00   9220400.0          2.9719           3.0355   \n",
       "2000-01-14 00:00:00   3485500.0          3.1926           3.2128   \n",
       "2000-01-18 00:00:00   4099800.0          3.2270           3.2450   \n",
       "2000-01-19 00:00:00   5336100.0          3.3211           3.3934   \n",
       "2000-01-20 00:00:00  16349400.0          3.6466           3.7108   \n",
       "2000-01-21 00:00:00   4427900.0          3.5402           3.6707   \n",
       "2000-01-24 00:00:00   3936400.0          3.3773           3.4840   \n",
       "2000-01-25 00:00:00   4438800.0          3.2890           3.3735   \n",
       "2000-01-26 00:00:00   3278200.0          3.5261           3.5341   \n",
       "2000-01-27 00:00:00   3037000.0          3.4377           3.4959   \n",
       "2000-01-28 00:00:00   3779900.0          3.2328           3.4760   \n",
       "2000-01-31 00:00:00   6265000.0          3.0361           3.2450   \n",
       "2000-02-01 00:00:00   2839600.0          3.2128           3.3414   \n",
       "2000-02-02 00:00:00   4144600.0          3.1165           3.2369   \n",
       "2000-02-03 00:00:00   4242800.0          3.2209           3.2228   \n",
       "2000-02-04 00:00:00   3797500.0          3.3292           3.3394   \n",
       "2000-02-07 00:00:00   3938100.0          3.4037           3.4699   \n",
       "2000-02-08 00:00:00   3648600.0          3.5743           3.6626   \n",
       "2000-02-09 00:00:00   2672900.0          3.6125           3.6665   \n",
       "2000-02-10 00:00:00   2705200.0          3.5341           3.6263   \n",
       "2000-02-11 00:00:00   1895100.0          3.4779           3.6504   \n",
       "2000-02-14 00:00:00   3281600.0          3.4898           3.5120   \n",
       "...                         ...             ...              ...   \n",
       "2018-02-13 00:00:00  32104756.0        161.6500         161.9500   \n",
       "2018-02-14 00:00:00  39669178.0        162.8800         163.0450   \n",
       "2018-02-15 00:00:00  50609595.0        169.0000         169.7900   \n",
       "2018-02-16 00:00:00  39638793.0        171.7700         172.3600   \n",
       "2018-02-20 00:00:00  33531012.0        171.4200         172.0500   \n",
       "2018-02-21 00:00:00  35833514.0        171.0100         172.8300   \n",
       "2018-02-22 00:00:00  30504116.0        171.7100         171.8000   \n",
       "2018-02-23 00:00:00  33329232.0        173.5400         173.6700   \n",
       "2018-02-26 00:00:00  36886432.0        176.2100         176.3500   \n",
       "2018-02-27 00:00:00  38685165.0        178.1600         179.1000   \n",
       "2018-02-28 00:00:00  33604574.0        178.0500         179.2600   \n",
       "2018-03-01 00:00:00  48801970.0        172.6600         178.5400   \n",
       "2018-03-02 00:00:00  38453950.0        172.4500         172.8000   \n",
       "2018-03-05 00:00:00  28401366.0        174.5200         175.2100   \n",
       "2018-03-06 00:00:00  23788506.0        176.1300         177.9100   \n",
       "2018-03-07 00:00:00  31703462.0        174.2700         174.9400   \n",
       "2018-03-08 00:00:00  23163767.0        175.0700         175.4800   \n",
       "2018-03-09 00:00:00  31385134.0        177.3900         177.9600   \n",
       "2018-03-12 00:00:00  32055405.0        180.2100         180.2900   \n",
       "2018-03-13 00:00:00  31168404.0        179.2400         182.5900   \n",
       "2018-03-14 00:00:00  29075469.0        177.8100         180.3200   \n",
       "2018-03-15 00:00:00  22584565.0        178.0701         178.5000   \n",
       "2018-03-16 00:00:00  36836456.0        177.6200         178.6500   \n",
       "2018-03-19 00:00:00  32804695.0        173.6600         177.3200   \n",
       "2018-03-20 00:00:00  19314039.0        174.9400         175.2400   \n",
       "2018-03-21 00:00:00  35247358.0        171.2600         175.0400   \n",
       "2018-03-22 00:00:00  41051076.0        168.6000         170.0000   \n",
       "2018-03-23 00:00:00  40248954.0        164.9400         168.3900   \n",
       "2018-03-26 00:00:00  36272617.0        166.4400         168.0700   \n",
       "2018-03-27 00:00:00  38962839.0        166.9200         173.6800   \n",
       "\n",
       "                     adj_high_price  \n",
       "price_date                           \n",
       "2000-01-03 00:00:00          3.6145  \n",
       "2000-01-04 00:00:00          3.5541  \n",
       "2000-01-05 00:00:00          3.5521  \n",
       "2000-01-06 00:00:00          3.4377  \n",
       "2000-01-07 00:00:00          3.2450  \n",
       "2000-01-10 00:00:00          3.2851  \n",
       "2000-01-11 00:00:00          3.1926  \n",
       "2000-01-12 00:00:00          3.0683  \n",
       "2000-01-13 00:00:00          3.1727  \n",
       "2000-01-14 00:00:00          3.2851  \n",
       "2000-01-18 00:00:00          3.4056  \n",
       "2000-01-19 00:00:00          3.4940  \n",
       "2000-01-20 00:00:00          3.9036  \n",
       "2000-01-21 00:00:00          3.6707  \n",
       "2000-01-24 00:00:00          3.6225  \n",
       "2000-01-25 00:00:00          3.6344  \n",
       "2000-01-26 00:00:00          3.6688  \n",
       "2000-01-27 00:00:00          3.6305  \n",
       "2000-01-28 00:00:00          3.5621  \n",
       "2000-01-31 00:00:00          3.3372  \n",
       "2000-02-01 00:00:00          3.3735  \n",
       "2000-02-02 00:00:00          3.2810  \n",
       "2000-02-03 00:00:00          3.3494  \n",
       "2000-02-04 00:00:00          3.5341  \n",
       "2000-02-07 00:00:00          3.6707  \n",
       "2000-02-08 00:00:00          3.7308  \n",
       "2000-02-09 00:00:00          3.7629  \n",
       "2000-02-10 00:00:00          3.6585  \n",
       "2000-02-11 00:00:00          3.6665  \n",
       "2000-02-14 00:00:00          3.7227  \n",
       "...                             ...  \n",
       "2018-02-13 00:00:00        164.7500  \n",
       "2018-02-14 00:00:00        167.5400  \n",
       "2018-02-15 00:00:00        173.0900  \n",
       "2018-02-16 00:00:00        174.8200  \n",
       "2018-02-20 00:00:00        174.2600  \n",
       "2018-02-21 00:00:00        174.1200  \n",
       "2018-02-22 00:00:00        173.9500  \n",
       "2018-02-23 00:00:00        175.6500  \n",
       "2018-02-26 00:00:00        179.3900  \n",
       "2018-02-27 00:00:00        180.4800  \n",
       "2018-02-28 00:00:00        180.6150  \n",
       "2018-03-01 00:00:00        179.7750  \n",
       "2018-03-02 00:00:00        176.3000  \n",
       "2018-03-05 00:00:00        177.7400  \n",
       "2018-03-06 00:00:00        178.2500  \n",
       "2018-03-07 00:00:00        175.8500  \n",
       "2018-03-08 00:00:00        177.1200  \n",
       "2018-03-09 00:00:00        180.0000  \n",
       "2018-03-12 00:00:00        182.3900  \n",
       "2018-03-13 00:00:00        183.5000  \n",
       "2018-03-14 00:00:00        180.5200  \n",
       "2018-03-15 00:00:00        180.2400  \n",
       "2018-03-16 00:00:00        179.1200  \n",
       "2018-03-19 00:00:00        177.4700  \n",
       "2018-03-20 00:00:00        176.8000  \n",
       "2018-03-21 00:00:00        175.0900  \n",
       "2018-03-22 00:00:00        172.6800  \n",
       "2018-03-23 00:00:00        169.9200  \n",
       "2018-03-26 00:00:00        173.1000  \n",
       "2018-03-27 00:00:00        175.1500  \n",
       "\n",
       "[4585 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(index=str, columns={\"adj_open_price\": \"volume\", \"adj_close_price\": \"adj_open_price\", \"adj_high_price\": \"adj_close_price\", \"adj_low_price\": \"adj_high_price\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(stock, seq_len):\n",
    "    amount_of_features = len(stock.columns)\n",
    "    data = stock.as_matrix() #pd.DataFrame(stock)\n",
    "    sequence_length = seq_len + 1\n",
    "    result = []\n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result.append(data[index: index + sequence_length])\n",
    "\n",
    "    result = np.array(result)\n",
    "    row = round(0.9 * result.shape[0])\n",
    "    train = result[:int(row), :]\n",
    "    x_train = train[:, :-1]\n",
    "    y_train = train[:, -1][:,-1]\n",
    "    x_test = result[int(row):, :-1]\n",
    "    y_test = result[int(row):, -1][:,-1]\n",
    "\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], amount_of_features))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], amount_of_features))  \n",
    "\n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(\n",
    "        input_dim=layers[0],\n",
    "        output_dim=layers[1],\n",
    "        return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "        layers[2],\n",
    "        return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(\n",
    "        output_dim=layers[2]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    start = time.time()\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\",metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model\n",
    "\n",
    "def build_model2(layers):\n",
    "        d = 0.2\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(128, input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "        model.add(Dropout(d))\n",
    "        model.add(LSTM(64, input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "        model.add(Dropout(d))\n",
    "        model.add(Dense(16,init='uniform',activation='relu'))        \n",
    "        model.add(Dense(1,init='uniform',activation='linear'))\n",
    "        model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (4106, 22, 4)\n",
      "y_train (4106,)\n",
      "X_test (456, 22, 4)\n",
      "y_test (456,)\n"
     ]
    }
   ],
   "source": [
    "window = 22\n",
    "X_train, y_train, X_test, y_test = load_data(df[::-1], window)\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Home/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(16, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "/Users/Home/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:32: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"linear\", kernel_initializer=\"uniform\")`\n"
     ]
    }
   ],
   "source": [
    "# model = build_model([3,lag,1])\n",
    "model = build_model2([4,window,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Home/anaconda/lib/python3.6/site-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3695 samples, validate on 411 samples\n",
      "Epoch 1/100\n",
      "3695/3695 [==============================] - 5s 1ms/step - loss: 4998.2648 - acc: 0.0000e+00 - val_loss: 1.2843 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "3695/3695 [==============================] - 3s 931us/step - loss: 4985.2652 - acc: 0.0000e+00 - val_loss: 0.8996 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "3695/3695 [==============================] - 3s 939us/step - loss: 4959.4371 - acc: 0.0000e+00 - val_loss: 0.3762 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "3695/3695 [==============================] - 3s 924us/step - loss: 4913.5197 - acc: 0.0000e+00 - val_loss: 0.0683 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "3695/3695 [==============================] - 4s 952us/step - loss: 4844.8387 - acc: 0.0000e+00 - val_loss: 0.6897 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "3695/3695 [==============================] - 3s 935us/step - loss: 4761.7885 - acc: 0.0000e+00 - val_loss: 3.0290 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "3695/3695 [==============================] - 3s 946us/step - loss: 4662.0291 - acc: 0.0000e+00 - val_loss: 8.1178 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "3695/3695 [==============================] - 4s 974us/step - loss: 4543.4627 - acc: 0.0000e+00 - val_loss: 17.5273 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "3695/3695 [==============================] - 3s 931us/step - loss: 4408.5813 - acc: 0.0000e+00 - val_loss: 33.0269 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "3695/3695 [==============================] - 4s 976us/step - loss: 4258.2811 - acc: 0.0000e+00 - val_loss: 56.7037 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "3695/3695 [==============================] - 3s 931us/step - loss: 4091.9767 - acc: 0.0000e+00 - val_loss: 90.8478 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "3695/3695 [==============================] - 4s 949us/step - loss: 3917.3330 - acc: 0.0000e+00 - val_loss: 137.6736 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "3695/3695 [==============================] - 4s 948us/step - loss: 3737.6159 - acc: 0.0000e+00 - val_loss: 198.9875 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "3695/3695 [==============================] - 3s 931us/step - loss: 3554.7813 - acc: 0.0000e+00 - val_loss: 276.3698 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "3695/3695 [==============================] - 4s 978us/step - loss: 3377.9179 - acc: 0.0000e+00 - val_loss: 371.1265 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "3695/3695 [==============================] - 4s 948us/step - loss: 3200.7589 - acc: 0.0000e+00 - val_loss: 485.2671 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "3695/3695 [==============================] - 4s 968us/step - loss: 3035.0131 - acc: 0.0000e+00 - val_loss: 615.6702 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "3695/3695 [==============================] - 4s 1ms/step - loss: 2884.2162 - acc: 0.0000e+00 - val_loss: 764.4379 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "3695/3695 [==============================] - 4s 975us/step - loss: 2756.0346 - acc: 0.0000e+00 - val_loss: 926.5126 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "3695/3695 [==============================] - 4s 954us/step - loss: 2634.2337 - acc: 0.0000e+00 - val_loss: 1098.3318 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "3695/3695 [==============================] - 4s 948us/step - loss: 2531.3605 - acc: 0.0000e+00 - val_loss: 1276.9302 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "3695/3695 [==============================] - 4s 971us/step - loss: 2443.5718 - acc: 0.0000e+00 - val_loss: 1461.1268 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "3695/3695 [==============================] - 3s 934us/step - loss: 2389.1853 - acc: 0.0000e+00 - val_loss: 1646.2427 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "3695/3695 [==============================] - 4s 949us/step - loss: 2336.5397 - acc: 0.0000e+00 - val_loss: 1820.3649 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "3695/3695 [==============================] - 4s 1ms/step - loss: 2304.4356 - acc: 0.0000e+00 - val_loss: 1980.7902 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "3695/3695 [==============================] - 4s 1ms/step - loss: 2267.5030 - acc: 0.0000e+00 - val_loss: 2127.6550 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "3695/3695 [==============================] - 5s 1ms/step - loss: 2253.7357 - acc: 0.0000e+00 - val_loss: 2240.6433 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "3695/3695 [==============================] - 4s 1ms/step - loss: 2256.5970 - acc: 0.0000e+00 - val_loss: 2322.9614 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "3695/3695 [==============================] - 4s 958us/step - loss: 2242.4541 - acc: 0.0000e+00 - val_loss: 2404.6550 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "3695/3695 [==============================] - 4s 957us/step - loss: 2242.1540 - acc: 0.0000e+00 - val_loss: 2483.7637 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "3695/3695 [==============================] - 4s 1ms/step - loss: 2241.2984 - acc: 0.0000e+00 - val_loss: 2532.2649 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "3695/3695 [==============================] - 4s 973us/step - loss: 2228.7856 - acc: 0.0000e+00 - val_loss: 2566.3679 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "3695/3695 [==============================] - 4s 960us/step - loss: 2238.4385 - acc: 0.0000e+00 - val_loss: 2583.5088 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "3695/3695 [==============================] - 4s 1ms/step - loss: 2241.8893 - acc: 0.0000e+00 - val_loss: 2592.2930 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "3695/3695 [==============================] - 4s 950us/step - loss: 2236.8241 - acc: 0.0000e+00 - val_loss: 2615.0232 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "3695/3695 [==============================] - 3s 927us/step - loss: 2238.4989 - acc: 0.0000e+00 - val_loss: 2633.0168 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "3695/3695 [==============================] - 3s 934us/step - loss: 2253.7044 - acc: 0.0000e+00 - val_loss: 2637.0090 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "3695/3695 [==============================] - 4s 986us/step - loss: 2244.1163 - acc: 0.0000e+00 - val_loss: 2635.0745 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "3695/3695 [==============================] - 3s 926us/step - loss: 2243.3968 - acc: 0.0000e+00 - val_loss: 2621.6531 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "3695/3695 [==============================] - 3s 941us/step - loss: 2241.2509 - acc: 0.0000e+00 - val_loss: 2626.1118 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "3695/3695 [==============================] - 4s 958us/step - loss: 2243.8537 - acc: 0.0000e+00 - val_loss: 2635.3003 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "3695/3695 [==============================] - 3s 936us/step - loss: 2237.4486 - acc: 0.0000e+00 - val_loss: 2629.0813 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "3695/3695 [==============================] - 3s 925us/step - loss: 2242.9503 - acc: 0.0000e+00 - val_loss: 2632.8152 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "3695/3695 [==============================] - 3s 930us/step - loss: 2247.5644 - acc: 0.0000e+00 - val_loss: 2636.5178 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "3695/3695 [==============================] - 4s 957us/step - loss: 2231.4308 - acc: 0.0000e+00 - val_loss: 2645.1680 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "3695/3695 [==============================] - 3s 926us/step - loss: 2226.8004 - acc: 0.0000e+00 - val_loss: 2645.1978 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "3695/3695 [==============================] - 3s 938us/step - loss: 2243.4721 - acc: 0.0000e+00 - val_loss: 2635.6638 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "3695/3695 [==============================] - 3s 936us/step - loss: 2238.0027 - acc: 0.0000e+00 - val_loss: 2625.6362 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "3695/3695 [==============================] - 3s 941us/step - loss: 2239.4829 - acc: 0.0000e+00 - val_loss: 2611.3301 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "3695/3695 [==============================] - 3s 945us/step - loss: 2234.6323 - acc: 0.0000e+00 - val_loss: 2616.1072 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "3695/3695 [==============================] - 3s 926us/step - loss: 2245.4581 - acc: 0.0000e+00 - val_loss: 2617.1040 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "3695/3695 [==============================] - 3s 930us/step - loss: 2239.3157 - acc: 0.0000e+00 - val_loss: 2626.0046 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "3695/3695 [==============================] - 4s 956us/step - loss: 2230.2816 - acc: 0.0000e+00 - val_loss: 2625.6406 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "3695/3695 [==============================] - 3s 938us/step - loss: 2241.8440 - acc: 0.0000e+00 - val_loss: 2619.8574 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "3695/3695 [==============================] - 5s 1ms/step - loss: 2241.6187 - acc: 0.0000e+00 - val_loss: 2611.8406 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "3695/3695 [==============================] - 3s 936us/step - loss: 2239.4528 - acc: 0.0000e+00 - val_loss: 2601.6902 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "3695/3695 [==============================] - 3s 932us/step - loss: 2235.7734 - acc: 0.0000e+00 - val_loss: 2601.8660 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "3695/3695 [==============================] - 3s 923us/step - loss: 2236.1268 - acc: 0.0000e+00 - val_loss: 2607.0417 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "3695/3695 [==============================] - 3s 943us/step - loss: 2243.1703 - acc: 0.0000e+00 - val_loss: 2614.3289 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "3695/3695 [==============================] - 3s 938us/step - loss: 2235.8116 - acc: 0.0000e+00 - val_loss: 2625.8809 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "3695/3695 [==============================] - 4s 957us/step - loss: 2240.1010 - acc: 0.0000e+00 - val_loss: 2646.6184 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "3695/3695 [==============================] - 4s 953us/step - loss: 2236.5983 - acc: 0.0000e+00 - val_loss: 2631.7302 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "3695/3695 [==============================] - 3s 933us/step - loss: 2235.6887 - acc: 0.0000e+00 - val_loss: 2633.9822 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "3695/3695 [==============================] - 3s 947us/step - loss: 2244.1925 - acc: 0.0000e+00 - val_loss: 2633.3564 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      "3695/3695 [==============================] - 3s 932us/step - loss: 2236.1296 - acc: 0.0000e+00 - val_loss: 2631.6223 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "3695/3695 [==============================] - 3s 943us/step - loss: 2242.9914 - acc: 0.0000e+00 - val_loss: 2640.2126 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "3695/3695 [==============================] - 3s 934us/step - loss: 2240.2175 - acc: 0.0000e+00 - val_loss: 2647.4526 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "3695/3695 [==============================] - 3s 947us/step - loss: 2240.6155 - acc: 0.0000e+00 - val_loss: 2644.8416 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "3695/3695 [==============================] - 4s 1ms/step - loss: 2239.5833 - acc: 0.0000e+00 - val_loss: 2667.9768 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "3695/3695 [==============================] - 3s 939us/step - loss: 2240.7725 - acc: 0.0000e+00 - val_loss: 2652.6042 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "3695/3695 [==============================] - 4s 983us/step - loss: 2245.4973 - acc: 0.0000e+00 - val_loss: 2651.3503 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      "3695/3695 [==============================] - 4s 964us/step - loss: 2249.9004 - acc: 0.0000e+00 - val_loss: 2640.3035 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "3695/3695 [==============================] - 3s 936us/step - loss: 2237.3554 - acc: 0.0000e+00 - val_loss: 2659.6423 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "3695/3695 [==============================] - 4s 978us/step - loss: 2238.2459 - acc: 0.0000e+00 - val_loss: 2701.2610 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "3695/3695 [==============================] - 4s 975us/step - loss: 2241.4538 - acc: 0.0000e+00 - val_loss: 2693.1978 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      "3695/3695 [==============================] - 3s 933us/step - loss: 2235.4347 - acc: 0.0000e+00 - val_loss: 2670.2861 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "3695/3695 [==============================] - 4s 952us/step - loss: 2237.1688 - acc: 0.0000e+00 - val_loss: 2633.9910 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "3695/3695 [==============================] - 3s 924us/step - loss: 2228.6087 - acc: 0.0000e+00 - val_loss: 2626.0286 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "3695/3695 [==============================] - 4s 970us/step - loss: 2248.0737 - acc: 0.0000e+00 - val_loss: 2613.8330 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "3695/3695 [==============================] - 4s 959us/step - loss: 2233.1652 - acc: 0.0000e+00 - val_loss: 2625.8901 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      "3695/3695 [==============================] - 3s 930us/step - loss: 2240.4582 - acc: 0.0000e+00 - val_loss: 2635.9954 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "3695/3695 [==============================] - 4s 956us/step - loss: 2243.6525 - acc: 0.0000e+00 - val_loss: 2641.8293 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      "3695/3695 [==============================] - 4s 985us/step - loss: 2249.9254 - acc: 0.0000e+00 - val_loss: 2648.5171 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "3695/3695 [==============================] - 3s 934us/step - loss: 2231.5829 - acc: 0.0000e+00 - val_loss: 2667.7783 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "3695/3695 [==============================] - 4s 951us/step - loss: 2232.4047 - acc: 0.0000e+00 - val_loss: 2675.9026 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "3695/3695 [==============================] - 4s 957us/step - loss: 2243.6644 - acc: 0.0000e+00 - val_loss: 2658.9333 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "3695/3695 [==============================] - 4s 965us/step - loss: 2237.3861 - acc: 0.0000e+00 - val_loss: 2623.2356 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      "3695/3695 [==============================] - 4s 977us/step - loss: 2238.2609 - acc: 0.0000e+00 - val_loss: 2616.9509 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      "3695/3695 [==============================] - 4s 965us/step - loss: 2236.5622 - acc: 0.0000e+00 - val_loss: 2636.2043 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "3695/3695 [==============================] - 3s 945us/step - loss: 2238.1587 - acc: 0.0000e+00 - val_loss: 2650.5654 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "3695/3695 [==============================] - 4s 961us/step - loss: 2236.4557 - acc: 0.0000e+00 - val_loss: 2639.7385 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      "3695/3695 [==============================] - 4s 953us/step - loss: 2240.9269 - acc: 0.0000e+00 - val_loss: 2627.0479 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      "3695/3695 [==============================] - 3s 936us/step - loss: 2242.3441 - acc: 0.0000e+00 - val_loss: 2625.4822 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      "3695/3695 [==============================] - 4s 957us/step - loss: 2241.7284 - acc: 0.0000e+00 - val_loss: 2627.2756 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "3695/3695 [==============================] - 3s 940us/step - loss: 2251.0900 - acc: 0.0000e+00 - val_loss: 2611.9126 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "3695/3695 [==============================] - 4s 997us/step - loss: 2241.6704 - acc: 0.0000e+00 - val_loss: 2591.3672 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      "3695/3695 [==============================] - 4s 963us/step - loss: 2243.6405 - acc: 0.0000e+00 - val_loss: 2599.8408 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "3695/3695 [==============================] - 3s 946us/step - loss: 2244.9665 - acc: 0.0000e+00 - val_loss: 2621.3337 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "3695/3695 [==============================] - 3s 945us/step - loss: 2238.2940 - acc: 0.0000e+00 - val_loss: 2611.5269 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      "3695/3695 [==============================] - 4s 963us/step - loss: 2240.5594 - acc: 0.0000e+00 - val_loss: 2598.6904 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1228da7b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=512,\n",
    "    nb_epoch=100,\n",
    "    validation_split=0.1,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 2266.53 MSE (47.61 RMSE)\n",
      "Test Score: 2491.44 MSE (49.91 RMSE)\n"
     ]
    }
   ],
   "source": [
    "trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(X_test[-1])\n",
    "diff=[]\n",
    "ratio=[]\n",
    "p = model.predict(X_test)\n",
    "for u in range(len(y_test)):\n",
    "    pr = p[u][0]\n",
    "    ratio.append((y_test[u]/pr)-1)\n",
    "    diff.append(abs(y_test[u]- pr))\n",
    "    #print(u, y_test[u], pr, (y_test[u]/pr)-1, abs(y_test[u]- pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFOgiIjGhQBcRiQkFuohITCjQRURi4v8BQByUur1VsB0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x124a6d240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt2\n",
    "\n",
    "plt2.plot(p,color='red', label='prediction')\n",
    "plt2.plot(y_test,color='blue', label='y_test')\n",
    "plt2.legend(loc='upper left')\n",
    "plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
